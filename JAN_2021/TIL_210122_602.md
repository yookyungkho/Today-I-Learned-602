# [210122 TIL] Today I Learned

_written by 602_

<br/>



---



## 1. Pytorch 필수 노트



학회에서 제공받은 <[파이썬 딥러닝 파이토치](https://book.naver.com/bookdb/book_detail.nhn?bid=16778336)> 책으로 파이토치 다시 한 번 제대로 공부해보기!

<br/>



### 1.1. tensor 연산

```python
import torch

tensor1 = torch.tensor([[[1., 2.],
                         [3., 4.]],
                        [[5., 6.],
                         [7., 8.]]]) # torch.randn(2,2,2)
tensor2 = torch.tensor([[[9., 10.],
                         [3., 4.]],
                        [[5., 6.],
                         [7., 8.]]])
# add
torch.add(tensor1, tensor2)
# sub
torch.sub(tensor1, tensor2)
# mul(element wise)
torch.mul(tensor1, tensor2)
# div
torch.div(tensor1, tensor2)
# matmul(행렬곱)
torch.matmul(tensor1, tensor2) # torch.mm(tensor1, tensor2)
```



### 1.2. GPU 설정

```python
if torch.cuda.is_available():
    DEVICE = torch.device('cuda')
else:
    DEVICE = torch.device('cpu')
```



### 1.3. `torch.radn`로 딥러닝 학습에 필요한 tensor 생성

```python
# x(input), y(output) : requires_grad = False
x = torch.randn(batch_size, input_size,
                device = DEVICE, dtype = torch.float, requires_grad = False)
y = torch torch.randn(batch_size, output_size,
                device = DEVICE, dtype = torch.float, requires_grad = False)

# w1, w2 (weights) : requires_grad = True -> gradient 계산
w1 = torch.randn(input_size, hidden_size,
                device = DEVICE, dtype = torch.float, requires_grad = True)
w2 = torch.randn(hidden_size, output_size,
                device = DEVICE, dtype = torch.float, requires_grad = True)
```



### 1.4. training iteration

`clamp()`, `mm()`, `pow()` `backward()`, `item()`, `torch.no_grad()`, `grad()`, `grad.zero_()`

```python
for iters in range(1, 501): #500 iteration
    y_pred = x.mm(w1).clamp(min=0).mm(w2) # w2*(non-linear(w1*x))
    ## mm: 행렬곱, clamp: non-linear function(0보다 큰 값은 그대로, 나머지는 0)
    loss = (y_pred - y).pow(2).sum #(y_pred - y)^2 합
    ## pow(): 지수를 취하는 기본 메서드, pow(2): 제곱차
    
    # 100 iter마다 loss 출력해보기
    if iters % 100 == 0:
        print("Iteration:", iters, "  Loss:", loss.item()) #tensor.item()
        
    # backpropagation 연산
    loss.backward()
    # 파라미터 업데이트
    with torch.no_grad(): # 업데이트 시점에서는 gradient 고정
        w1 -= LR * w1.grad # grad 반대방향으로 업데이트
        w2 -= LR * w2.grad
        # 다음 iters의 loss.backward()에서 gradient 새로 계산하므로 grad 0으로 초기화
        w1.grad.zero_()
        w2.grad.zero_()
```





### 1.5. 역전파 gradient 연산 복습





back propagation 연산과정은 주기적으로 복습해줘야 한다.

게을리 하면 어느 순간 게슈탈트 붕괴가 올 때가 있다..!

<br/>

> 순전파(예시 - bias 생략):
>
> `w1*x` -> `sigmoid(w1*x)` -> `w2*sigmoid(w1*x)` -> `softmax(w2*sigmoid(w1*x))` -> `Loss`
>
> 역전파 gradient 연산 순서:
>
> `Loss` -> `softmax(w2*sigmoid(w1*x))` -> `w2*sigmoid(w1*x)` -> `sigmoid(w1*x)` -> `w1*x` -> `w1`