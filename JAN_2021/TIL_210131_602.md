# [210131 TIL] CS231n Lecture 6 - Training Neural Networks 1

_written by 602_

<br/>



---

cs231n ë³µìŠµí•˜ê¸°

ì§€ë‚œ í•™ê¸°, íˆ¬ë¹…ìŠ¤ ì´ë¯¸ì§€ ì„¸ë¯¸ë‚˜ë¥¼ ì§„í–‰í•˜ë©´ì„œ cs231nì„ í•œë²ˆ ì­‰ ìˆ˜ê°•í•˜ë©° í•™íšŒì›ë“¤ê³¼ [gitbook](https://tobigs-staff.gitbook.io/-1/)ì— ìë£Œì •ë¦¬ë¥¼ í–ˆê¸° ë•Œë¬¸ì—, ìƒˆë¡œ í•©ë¥˜í•œ ë”¥ëŸ¬ë‹ ìŠ¤í„°ë””ì—ì„œëŠ” ê¸°ì–µí•´ì•¼í•  ë‚´ìš© ìœ„ì£¼ë¡œ ê°„ëµí•˜ê²Œ ì •ë¦¬í•˜ì˜€ë‹¤.

 

<br/>



---

# ğŸ‘€CS231n. Lecture 6) Training Neural Networks 1



## 1. Activation Functions



### Sigmoid

- 0ê³¼ 1 ì‚¬ì´ì˜ ê°’ì„ ë°˜í™˜
- ë¬¸ì œ
  - 1. saturated neurons kill the gradients(ê°’ì´ ì¡°ê¸ˆë§Œ ì»¤ì ¸ë„ gradientê°€ 0ì— ê°€ê¹Œìš´ ê°’ì„ ê°€ì§ -> ê¸°ìš¸ê¸° ì†Œì‹¤)
  - 2. x zero-centered(ì´ì „ layerì—ì„œ ì˜¤ëŠ” inputì´ ì „ë¶€ ì–‘ìˆ˜ -> gradientsê°€ all positive or all negative)



### Tanh

- -1ê³¼ 1 ì‚¬ì´ì˜ ê°’ì„ ë°˜í™˜ -> zero centered
- ë¬¸ì œ: sigmoid ë¬¸ì œì™€ ê°€í‹° ì—¬ì „í•œ saturate(-> ê¸°ìš¸ê¸° ì†Œì‹¤)



### ReLU

- ìŒìˆ˜ -> gradient 0, ì–‘ìˆ˜ -> gradient 1 => x saturate
- ë¬¸ì œ: ì—¬ì „íˆ x zero centered(Dead ReLU - update ë˜ì§€ ì•Šê³  êº¼ì ¸ë²„ë¦¼, normalization ë“±ì˜ ë°©ë²•ìœ¼ë¡œ ì¸í•´ ì¼ë°˜ì ìœ¼ë¡œ í° ë¬¸ì œê°€ ë˜ì§€ëŠ” ì•ŠìŒ)
- Dead ReLuì˜ í•´ê²°ì±…
  - Leaky ReLU : $f(x) = max(0.01,x)$
  - ELU: leakyë³´ë‹¤ noiseì— ëŒ€í•˜ì—¬ robust
  - GeLU(generalized exponential), ReLU, Leaky ReLU ê°€ ë…¼ë¬¸ì—ì„œ ì¢…ì¢… ë³´ì´ëŠ”ê±°ê°™ìŒ

<br/>



**ê²°ë¡ **: ì›¬ë§Œí•˜ë©´ **ReLU**ë¥¼ ì“°ì

<br/>



## 2. Data Preprocessing



### Standardization, Normalization

- cf) **whitening**
  - inputì˜ featureë“¤ì„ uncorrelatedí•˜ê²Œ ë§Œë“¤ì–´ì£¼ê³  ê°ê°ì˜ varianceë¥¼ 1ë¡œ ë§Œë“œëŠ” ì‘ì—…
  - segmentationì—ì„œëŠ” ì“°ë©´ ì•ˆë¨.(ì´ë¯¸ì§€ ë‚´ ìœ„ì¹˜ì— ë”°ë¼ ê´€ê³„ê°€ ìˆì„ ìˆ˜ ìˆê¸° ë•Œë¬¸)



## 3. Weight Initialization

### W = 0?

- ë‰´ëŸ°ë“¤ ëª¨ë‘ ê°™ì€ ì—­í• (update in same way)

### W: small?

- (tahn ê¸°ì¤€) $A=WX+b$, $F=f(A)$
- ì²« ì¸µ ê²°ê³¼ë§Œ ì˜ ë‚˜ì˜¤ê³  ì¸µì„ ì§€ë‚  ìˆ˜ë¡ activation outputì´ ëª¨ë‘ 0ì— ê°€ê¹Œì›Œì§(ë¶„ì‚° -> 0)
- gradients($\frac{\partial A}{\partial W} = X$) -> 0

### W: big?

- activation outputì´ -1ê³¼ 1ì— ì ë ¤ë²„ë¦¼
- gradients vanishing ($\frac{\partial F}{\partial A}$ -> 0 )

 <br/>

---



**ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”ì˜ í•µì‹¬: back prop ì‹œì— ë„¤íŠ¸ì›Œí¬ê°€ ì£½ì§€ ì•Šë„ë¡ í‰ê·  0, ë¶„ì‚° 1ì„ ë§ì¶°ì£¼ì**



### Xavier Initialization

- 
- ReLUì™€ëŠ” ê°™ì´ ì“°ë©´ ì•ˆë¨

### He Initialization

- 



cf) ê°•í™”í•™ìŠµ - orthogonal initialization



## 4. Batch Normalization

**í•µì‹¬: keep activations in a gaussian range that we want**

- í‰ê·  0, ë¶„ì‚° 1ë¡œ mini-batch ë‚´ì—ì„œ ì •ê·œí™”

### Step1)

### Step2) 

### Step3) 

- ê°ë§ˆ, ë² íƒ€ëŠ” trainable parameter
- ê·¸ëŒ€ë¡œ ì“°ê³  ì‹¶ìœ¼ë©´ $\gamma$:1, $\beta$:0
- ì›ìƒë³µêµ¬ë¥¼ ì›í•˜ë©´ $\gamma$: ë¶„ì‚°, $\beta$: í‰ê·  

### Testì—ì„œëŠ”?

- running averagesë¥¼ ì´ìš©í•˜ì—¬



**cf) Layer normalization**

- resnetì˜ residual connectionì²˜ëŸ¼ layer ë‚´ì˜ 0~1 ë¶„í¬ê°€ ê¹¨ì¡Œì„ ë•Œ ì ìš©



**cf) normalization ì¶”ê°€**

- normalizationê³¼ whiteningì— ëŒ€í•´ ì •ë¦¬í•œ switchable whitening ë…¼ë¬¸ ì°¸ê³ í•˜ê¸°
- Batch-size trick: bactch sizeë¥¼ í‚¤ìš°ê³  ì‹¶ì§€ë§Œ ì»´í“¨í„° ì„±ëŠ¥ì´ ì¢‹ì§€ ì•Šì„ ë•Œ ì‚¬ìš©
  - mini batch ë³„ lossë¥¼ êµ¬í•œ ë’¤ ì—­ì „íŒŒ ê³¼ì • ê±°ì¹˜ì§€ ì•Šê³  lossë¥¼ ë‹¤ ë”í•´ì„œ í‰ê· ëƒ„
  - ì‹¤ì œ batch sizeê°€ í° ê²ƒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ë‚´ì§€ëŠ” ì•ŠìŒ. ë¹„ìŠ·í•œ íš¨ê³¼ë¥¼ ë‚¼ ë¿
  - batch sizeë¥¼ í‚¤ìš°ë©´ stableí•œ ì—…ë°ì´íŠ¸ ê°€ëŠ¥
  - ê°ˆìˆ˜ë¡ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì»¤ì§€ëŠ” ì¶”ì„¸



## 5. Hyper-Parameter Optimization

- **random search, grid search ëŒ€ì‹  ìš”ì¦˜ì€ Bayesian Optimization**

- rough range for LR: [1e-3, ... , 1e-5]
  - 









---

