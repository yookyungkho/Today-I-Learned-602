# [210131 TIL] CS231n Lecture 6 - Training Neural Networks 1

_written by 602_

<br/>



---

cs231n 복습하기

지난 학기, 투빅스 이미지 세미나를 진행하면서 cs231n을 한번 쭉 수강하며 학회원들과 [gitbook](https://tobigs-staff.gitbook.io/-1/)에 자료정리를 했기 때문에, 새로 합류한 딥러닝 스터디에서는 기억해야할 내용 위주로 간략하게 정리하였다.

 

<br/>



---

# 👀CS231n. Lecture 6) Training Neural Networks 1



## 1. Activation Functions



### Sigmoid

- 0과 1 사이의 값을 반환
- 문제
  - 1. saturated neurons kill the gradients(값이 조금만 커져도 gradient가 0에 가까운 값을 가짐 -> 기울기 소실)
  - 2. x zero-centered(이전 layer에서 오는 input이 전부 양수 -> gradients가 all positive or all negative)



### Tanh

- -1과 1 사이의 값을 반환 -> zero centered
- 문제: sigmoid 문제와 가티 여전한 saturate(-> 기울기 소실)



### ReLU

- 음수 -> gradient 0, 양수 -> gradient 1 => x saturate
- 문제: 여전히 x zero centered(Dead ReLU - update 되지 않고 꺼져버림, normalization 등의 방법으로 인해 일반적으로 큰 문제가 되지는 않음)
- Dead ReLu의 해결책
  - Leaky ReLU : $f(x) = max(0.01,x)$
  - ELU: leaky보다 noise에 대하여 robust
  - GeLU(generalized exponential), ReLU, Leaky ReLU 가 논문에서 종종 보이는거같음

<br/>



**결론**: 웬만하면 **ReLU**를 쓰자

<br/>



## 2. Data Preprocessing



### Standardization, Normalization

- cf) **whitening**
  - input의 feature들을 uncorrelated하게 만들어주고 각각의 variance를 1로 만드는 작업
  - segmentation에서는 쓰면 안됨.(이미지 내 위치에 따라 관계가 있을 수 있기 때문)



## 3. Weight Initialization

### W = 0?

- 뉴런들 모두 같은 역할(update in same way)

### W: small?

- (tahn 기준) $A=WX+b$, $F=f(A)$
- 첫 층 결과만 잘 나오고 층을 지날 수록 activation output이 모두 0에 가까워짐(분산 -> 0)
- gradients($\frac{\partial A}{\partial W} = X$) -> 0

### W: big?

- activation output이 -1과 1에 쏠려버림
- gradients vanishing ($\frac{\partial F}{\partial A}$ -> 0 )

 <br/>

---



**가중치 초기화의 핵심: back prop 시에 네트워크가 죽지 않도록 평균 0, 분산 1을 맞춰주자**



### Xavier Initialization

- 
- ReLU와는 같이 쓰면 안됨

### He Initialization

- 



cf) 강화학습 - orthogonal initialization



## 4. Batch Normalization

**핵심: keep activations in a gaussian range that we want**

- 평균 0, 분산 1로 mini-batch 내에서 정규화

### Step1)

### Step2) 

### Step3) 

- 감마, 베타는 trainable parameter
- 그대로 쓰고 싶으면 $\gamma$:1, $\beta$:0
- 원상복구를 원하면 $\gamma$: 분산, $\beta$: 평균 

### Test에서는?

- running averages를 이용하여



**cf) Layer normalization**

- resnet의 residual connection처럼 layer 내의 0~1 분포가 깨졌을 때 적용



**cf) normalization 추가**

- normalization과 whitening에 대해 정리한 switchable whitening 논문 참고하기
- Batch-size trick: bactch size를 키우고 싶지만 컴퓨터 성능이 좋지 않을 때 사용
  - mini batch 별 loss를 구한 뒤 역전파 과정 거치지 않고 loss를 다 더해서 평균냄
  - 실제 batch size가 큰 것과 같은 결과를 내지는 않음. 비슷한 효과를 낼 뿐
  - batch size를 키우면 stable한 업데이트 가능
  - 갈수록 배치 사이즈 커지는 추세



## 5. Hyper-Parameter Optimization

- **random search, grid search 대신 요즘은 Bayesian Optimization**

- rough range for LR: [1e-3, ... , 1e-5]
  - 









---

