# [210215 TIL] CS231n Lecture 9 - CNN Architectures

Today I Learned

_written by 602_

<br/>



---

**📕기초과목 제대로 다지기📕**

**: cs231n 복습**



지난 학기, 투빅스 이미지 세미나를 진행하면서 cs231n을 한번 쭉 수강하며 학회원들과 [gitbook](https://tobigs-staff.gitbook.io/-1/)에 자료정리를 했기 때문에, 새로 합류한 딥러닝 스터디에서는 기억해야할 내용 위주로 간략하게 정리하였다.

 

<br/>



---

# 👀CS231n. Lecture 9) CNN Architectures



<br/>



## 1. AlexNet

- CNN을 이용하여 ImageNET에서 우승한 최초의 모델
- 모델 구조 설계하려면 파라미터 개수도 계산할 줄 알아야함

<br/>



## 2. VGG

- 모델이 훨씬 깊어짐(Deeper Network)
- 3x3 Conv filter로 고정 -> why?
  - effective receptive field
    - 3x3을 3층 쌓은 것이 7x7 한 층과 같은 범위의 영역을 보면서 필요한 파라미터가 더 적음
- FC& (4096)이 이미지의 특성을 잘 반영하고 있어서 generalization에 유리했음

<br/>



## 3. GoogLeNet

**Deeper Networks, with Computational Efficiency**

- AlexNet에 비해 12배 적은 파라미터
- FC Layers 없음
- 특이한 구조 - **Inception Module**
  - 1x1 conv, 3x3 conv, 5x5 conv, 3x3 pool -> concat
    - 더 다양한 receptive field를 볼 수 있도록
  - prob: 너무 많은 연산량 (computational complexity) b/c concat
  - sol: Bottleneck 구조 -> 파라미터 수를 줄이고 더 중요한 feature들 뽑아내서 흘려보내주기
    - 1x1 conv bottleneck layers 추가
- 기울기 소실 방지를 위해 추가한 Auxiliary Classification
- 22 layers

<br/>



## 4. ResNet

- 엄청나게 깊어진 모델(152층)
- prob: 모델이 깊어질수록 overfitting 뿐만 아니라 optimization prob. 존재
- sol: identity mapping
  - gradient를 그대로 흘려주기 때문에 기울기 소실 방지
- 전부 3x3 conv layer로 구성(depth는 점점 늘어남)
- bottleneck(1x1) layer 포함(GoogLeNet과 동일)

- 사람도 이겨버린 놀라운 성능

<br/>



### Other Networks



**NiN(Network in Network)**

- MLP로 채움
- 구글넷의 시초(Bottleneck 개념 정립)

<br/>

**Wide Residual Networks**

- depth보다 residual이 중요하다
- filter 개수를 늘렸더니 50층만으로도 충분한 성능 기록(파라미터 수는 동일)

<br/>

**ResNeXt**

- parallel한 구조
- inception module과 유사

<br/>

**Squeeze Net**

- Squeeze layer + Expand Layer
- AlexNet만큼의 Accuracy, 파라미터는 1/50

<br/>

**cf) 1x1 conv를 활용하는 다양한 방식** - for Efficient DL

- Depthwise Seperable Convolution (최신의 효율적 방법 -> 필수)
- Inverted Residual Block (MobileNet V2)
- 경량화 공부할 때 CV 분야에서는 1x1 conv를 눈여겨볼 필요성 



---

