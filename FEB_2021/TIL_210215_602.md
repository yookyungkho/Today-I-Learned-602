# [210215 TIL] CS231n Lecture 9 - CNN Architectures

Today I Learned

_written by 602_

<br/>



---

**📕기초과목 제대로 다지기📕**

**: cs231n 복습**



지난 학기, 투빅스 이미지 세미나를 진행하면서 cs231n을 한번 쭉 수강하며 학회원들과 [gitbook](https://tobigs-staff.gitbook.io/-1/)에 자료정리를 했기 때문에, 새로 합류한 딥러닝 스터디에서는 기억해야할 내용 위주로 간략하게 정리하였다.

 

<br/>



---

# 👀CS231n. Lecture 9) CNN Architectures



<br/>



## 1. AlexNet

![image](https://user-images.githubusercontent.com/68496320/107973356-7c205b80-6ff8-11eb-82d4-c81065b71e8d.png)
- CNN을 이용하여 ImageNET에서 우승한 최초의 모델
- 모델 구조 설계하려면 파라미터 개수도 계산할 줄 알아야함

<br/>



## 2. VGG

![image](https://user-images.githubusercontent.com/68496320/107973376-804c7900-6ff8-11eb-886e-9db7878bac4c.png)
- 모델이 훨씬 깊어짐(Deeper Network)
- 3x3 Conv filter로 고정 -> why?
  - effective receptive field
    - 3x3을 3층 쌓은 것이 7x7 한 층과 같은 범위의 영역을 보면서 필요한 파라미터가 더 적음
- FC& (4096)이 이미지의 특성을 잘 반영하고 있어서 generalization에 유리했음

<br/>



## 3. GoogLeNet

**Deeper Networks, with Computational Efficiency**
![image](https://user-images.githubusercontent.com/68496320/107973396-83e00000-6ff8-11eb-866e-e361339164b2.png)
- AlexNet에 비해 12배 적은 파라미터
- FC Layers 없음
- 특이한 구조 - **Inception Module**
  - 1x1 conv, 3x3 conv, 5x5 conv, 3x3 pool -> concat
    - 더 다양한 receptive field를 볼 수 있도록
  - prob: 너무 많은 연산량 (computational complexity) b/c concat
  - sol: Bottleneck 구조 -> 파라미터 수를 줄이고 더 중요한 feature들 뽑아내서 흘려보내주기
    - 1x1 conv bottleneck layers 추가
- 기울기 소실 방지를 위해 추가한 Auxiliary Classification
- 22 layers

<br/>



## 4. ResNet

![image](https://user-images.githubusercontent.com/68496320/107973408-86daf080-6ff8-11eb-8a30-1a01a6863693.png)
- 엄청나게 깊어진 모델(152층)
- prob: 모델이 깊어질수록 overfitting 뿐만 아니라 optimization prob. 존재
- sol: identity mapping
  - gradient를 그대로 흘려주기 때문에 기울기 소실 방지
- 전부 3x3 conv layer로 구성(depth는 점점 늘어남)
- bottleneck(1x1) layer 포함(GoogLeNet과 동일)

- 사람도 이겨버린 놀라운 성능

<br/>



### Other Networks



**NiN(Network in Network)**
![image](https://user-images.githubusercontent.com/68496320/107973420-8a6e7780-6ff8-11eb-961c-bd88949a6984.png)
- MLP로 채움
- 구글넷의 시초(Bottleneck 개념 정립)

<br/>

**Wide Residual Networks**
![image](https://user-images.githubusercontent.com/68496320/107973431-8d696800-6ff8-11eb-9dd5-b7496b4367da.png)
- depth보다 residual이 중요하다
- filter 개수를 늘렸더니 50층만으로도 충분한 성능 기록(파라미터 수는 동일)

<br/>

**ResNeXt**
![image](https://user-images.githubusercontent.com/68496320/107973441-8fcbc200-6ff8-11eb-86a3-4ebef88ff8ba.png)
- parallel한 구조
- inception module과 유사

<br/>

**Squeeze Net**
![image](https://user-images.githubusercontent.com/68496320/107973453-92c6b280-6ff8-11eb-9202-5693daa26d19.png)
- Squeeze layer + Expand Layer
- AlexNet만큼의 Accuracy, 파라미터는 1/50

<br/>

**cf) 1x1 conv를 활용하는 다양한 방식** - for Efficient DL

- Depthwise Seperable Convolution (최신의 효율적 방법 -> 필수)
- Inverted Residual Block (MobileNet V2)
- 경량화 공부할 때 CV 분야에서는 1x1 conv를 눈여겨볼 필요성 



---

