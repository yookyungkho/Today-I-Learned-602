# [210215 TIL] CS231n Lecture 9 - CNN Architectures

Today I Learned

_written by 602_

<br/>



---

**ğŸ“•ê¸°ì´ˆê³¼ëª© ì œëŒ€ë¡œ ë‹¤ì§€ê¸°ğŸ“•**

**: cs231n ë³µìŠµ**



ì§€ë‚œ í•™ê¸°, íˆ¬ë¹…ìŠ¤ ì´ë¯¸ì§€ ì„¸ë¯¸ë‚˜ë¥¼ ì§„í–‰í•˜ë©´ì„œ cs231nì„ í•œë²ˆ ì­‰ ìˆ˜ê°•í•˜ë©° í•™íšŒì›ë“¤ê³¼ [gitbook](https://tobigs-staff.gitbook.io/-1/)ì— ìë£Œì •ë¦¬ë¥¼ í–ˆê¸° ë•Œë¬¸ì—, ìƒˆë¡œ í•©ë¥˜í•œ ë”¥ëŸ¬ë‹ ìŠ¤í„°ë””ì—ì„œëŠ” ê¸°ì–µí•´ì•¼í•  ë‚´ìš© ìœ„ì£¼ë¡œ ê°„ëµí•˜ê²Œ ì •ë¦¬í•˜ì˜€ë‹¤.

 

<br/>



---

# ğŸ‘€CS231n. Lecture 9) CNN Architectures



<br/>



## 1. AlexNet

![image](https://user-images.githubusercontent.com/68496320/107973356-7c205b80-6ff8-11eb-82d4-c81065b71e8d.png)
- CNNì„ ì´ìš©í•˜ì—¬ ImageNETì—ì„œ ìš°ìŠ¹í•œ ìµœì´ˆì˜ ëª¨ë¸
- ëª¨ë¸ êµ¬ì¡° ì„¤ê³„í•˜ë ¤ë©´ íŒŒë¼ë¯¸í„° ê°œìˆ˜ë„ ê³„ì‚°í•  ì¤„ ì•Œì•„ì•¼í•¨

<br/>



## 2. VGG

![image](https://user-images.githubusercontent.com/68496320/107973376-804c7900-6ff8-11eb-886e-9db7878bac4c.png)
- ëª¨ë¸ì´ í›¨ì”¬ ê¹Šì–´ì§(Deeper Network)
- 3x3 Conv filterë¡œ ê³ ì • -> why?
  - effective receptive field
    - 3x3ì„ 3ì¸µ ìŒ“ì€ ê²ƒì´ 7x7 í•œ ì¸µê³¼ ê°™ì€ ë²”ìœ„ì˜ ì˜ì—­ì„ ë³´ë©´ì„œ í•„ìš”í•œ íŒŒë¼ë¯¸í„°ê°€ ë” ì ìŒ
- FC& (4096)ì´ ì´ë¯¸ì§€ì˜ íŠ¹ì„±ì„ ì˜ ë°˜ì˜í•˜ê³  ìˆì–´ì„œ generalizationì— ìœ ë¦¬í–ˆìŒ

<br/>



## 3. GoogLeNet

**Deeper Networks, with Computational Efficiency**
![image](https://user-images.githubusercontent.com/68496320/107973396-83e00000-6ff8-11eb-866e-e361339164b2.png)
- AlexNetì— ë¹„í•´ 12ë°° ì ì€ íŒŒë¼ë¯¸í„°
- FC Layers ì—†ìŒ
- íŠ¹ì´í•œ êµ¬ì¡° - **Inception Module**
  - 1x1 conv, 3x3 conv, 5x5 conv, 3x3 pool -> concat
    - ë” ë‹¤ì–‘í•œ receptive fieldë¥¼ ë³¼ ìˆ˜ ìˆë„ë¡
  - prob: ë„ˆë¬´ ë§ì€ ì—°ì‚°ëŸ‰ (computational complexity) b/c concat
  - sol: Bottleneck êµ¬ì¡° -> íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ì¤„ì´ê³  ë” ì¤‘ìš”í•œ featureë“¤ ë½‘ì•„ë‚´ì„œ í˜ë ¤ë³´ë‚´ì£¼ê¸°
    - 1x1 conv bottleneck layers ì¶”ê°€
- ê¸°ìš¸ê¸° ì†Œì‹¤ ë°©ì§€ë¥¼ ìœ„í•´ ì¶”ê°€í•œ Auxiliary Classification
- 22 layers

<br/>



## 4. ResNet

![image](https://user-images.githubusercontent.com/68496320/107973408-86daf080-6ff8-11eb-8a30-1a01a6863693.png)
- ì—„ì²­ë‚˜ê²Œ ê¹Šì–´ì§„ ëª¨ë¸(152ì¸µ)
- prob: ëª¨ë¸ì´ ê¹Šì–´ì§ˆìˆ˜ë¡ overfitting ë¿ë§Œ ì•„ë‹ˆë¼ optimization prob. ì¡´ì¬
- sol: identity mapping
  - gradientë¥¼ ê·¸ëŒ€ë¡œ í˜ë ¤ì£¼ê¸° ë•Œë¬¸ì— ê¸°ìš¸ê¸° ì†Œì‹¤ ë°©ì§€
- ì „ë¶€ 3x3 conv layerë¡œ êµ¬ì„±(depthëŠ” ì ì  ëŠ˜ì–´ë‚¨)
- bottleneck(1x1) layer í¬í•¨(GoogLeNetê³¼ ë™ì¼)

- ì‚¬ëŒë„ ì´ê²¨ë²„ë¦° ë†€ë¼ìš´ ì„±ëŠ¥

<br/>



### Other Networks



**NiN(Network in Network)**
![image](https://user-images.githubusercontent.com/68496320/107973420-8a6e7780-6ff8-11eb-961c-bd88949a6984.png)
- MLPë¡œ ì±„ì›€
- êµ¬ê¸€ë„·ì˜ ì‹œì´ˆ(Bottleneck ê°œë… ì •ë¦½)

<br/>

**Wide Residual Networks**
![image](https://user-images.githubusercontent.com/68496320/107973431-8d696800-6ff8-11eb-9dd5-b7496b4367da.png)
- depthë³´ë‹¤ residualì´ ì¤‘ìš”í•˜ë‹¤
- filter ê°œìˆ˜ë¥¼ ëŠ˜ë ¸ë”ë‹ˆ 50ì¸µë§Œìœ¼ë¡œë„ ì¶©ë¶„í•œ ì„±ëŠ¥ ê¸°ë¡(íŒŒë¼ë¯¸í„° ìˆ˜ëŠ” ë™ì¼)

<br/>

**ResNeXt**
![image](https://user-images.githubusercontent.com/68496320/107973441-8fcbc200-6ff8-11eb-86a3-4ebef88ff8ba.png)
- parallelí•œ êµ¬ì¡°
- inception moduleê³¼ ìœ ì‚¬

<br/>

**Squeeze Net**
![image](https://user-images.githubusercontent.com/68496320/107973453-92c6b280-6ff8-11eb-9202-5693daa26d19.png)
- Squeeze layer + Expand Layer
- AlexNetë§Œí¼ì˜ Accuracy, íŒŒë¼ë¯¸í„°ëŠ” 1/50

<br/>

**cf) 1x1 convë¥¼ í™œìš©í•˜ëŠ” ë‹¤ì–‘í•œ ë°©ì‹** - for Efficient DL

- Depthwise Seperable Convolution (ìµœì‹ ì˜ íš¨ìœ¨ì  ë°©ë²• -> í•„ìˆ˜)
- Inverted Residual Block (MobileNet V2)
- ê²½ëŸ‰í™” ê³µë¶€í•  ë•Œ CV ë¶„ì•¼ì—ì„œëŠ” 1x1 convë¥¼ ëˆˆì—¬ê²¨ë³¼ í•„ìš”ì„± 



---

