

# [210214 TIL] Language Model 개념 정리

_written by 602_

<br/>



---



## 📗Language Model 개념 정리

[Reference Link](https://www.youtube.com/watch?v=ycxZORWPPP4&list=PLoYt3dmk_LdhWlr9vPwfZCgivawGtcTjk)



### Language Model



**Statistical Language Model**

- Sparsity problem: 새로운 단어 조합에 대해서는 확률값이 0이 되는 문제



**N-Gram Language Model**

- Uni-gram

  - 하나의 단어에 대한 확률만 계산 -> 웬만하면 확률값이 0이 되지는 않음
  - 단어의 순서 고려 못함

- Bi-gram

  - 두 단어씩 확률값 계산 ($P(내일|뭐먹지)$)
  - 여전한 Sparsity Problem ($P(배고파|시험)$)

  

**Sparsity Problem의 대안**

- Additive(Laplace) Smoothing: 단어들의 count에 1을 추가하여 확률이 0이 되는 것을 방지
- Backoff Model:  (Ex) Tri-gram에서 확률값이 0이 나왔을 경우 Bi-gram으로 변경하여 확률 계산
- Smoothed(Interpolated) Model: (Ex) Uni,Bi,Tri-gram의 확률값을 선형결합하여 최종 확률 계산

<br/>



### Word Representation



**One-hot Encoding**

- 단어를 벡터로 표현하는 가장 간단한 방법
- 단어의 인덱스에 해당하는 위치에 1, 나머지를 0으로 할당
- (-) 단어의 수에 비례하여 차원 증가 -> 연산 부담
- (-) 단어 간 관계 파악 불가(두 단어 벡터의 내적값, 즉 유사도는 0) -> 의미나 문맥 반영 불가



**Distributed Representation**

- 분산 표상(실수로 이루어진 벡터로 표현)
- Embedding이라고도 불림
- (+)적은 차원으로 mapping / 단어 간 관계 파악 가능 / 값의 변화에 덜 민감



**Embedding layer 소스 코드(Tensorflow)**

```python
class Embedding(Layer):
  def __init__(self,
input_dim,output_dim,embeddings_initializer='uniform',embeddings_regularizer=None,activity_regularizer=None,embeddings_constraint=None,
mask_zero=False,input_length=None,**kwargs):
    self.input_dim = input_dim
    self.output_dim = output_dim
    self.embeddings_initializer = initializers.get(embeddings_initializer)
    self.embeddings_regularizer = regularizers.get(embeddings_regularizer)
    self.activity_regularizer = regularizers.get(activity_regularizer)
    self.embeddings_constraint = constraints.get(embeddings_constraint)
    self.mask_zero = mask_zero
    self.supports_masking = mask_zero
    self.input_length = input_length
    self._supports_ragged_inputs = True

  @tf_utils.shape_type_conversion
  def build(self, input_shape):
        self.embeddings = self.add_weight(
            shape=(self.input_dim, self.output_dim),
            initializer=self.embeddings_initializer,
            name='embeddings',
            regularizer=self.embeddings_regularizer,
            constraint=self.embeddings_constraint)

  def call(self, inputs):
    out = embedding_ops.embedding_lookup(self.embeddings, inputs)
    return out
```



<br/>



### Neural Probabilistic Language Model



**Purpose**

- For Language Model(이하 LM): 단어 시퀀스에 대한 Joint Probability Function을 나타내는 것
- For Word Representation: 단어집합 내 각 단어의 Embedded feature를 학습하는 것

- 유사한 구조나 의미를 가진 문장에 대한 일반화 가능



**Architecture**

- 원핫벡터 -> Embedding (cf. Lookup Table/Shared Parameter) ->  FCL(hidden size) -> Tanh -> FCL(Vocab size)
- Lookup Table: `Vocab size` x `m(Embedding Size)`
- 목표: Log Likelihood를 최대화하는 파라미터를 찾는 것



**Evaluation Metric: Perplexity**

- 단어들의 Joint probability에 역수,n제곱근을 취한 값

  $\sqrt[n]{\frac{1}{P(W_1,W_2, ..., W_n)}}$

- LM이 주어진 문장을 헷갈려하는 정도

  - 문장이 헷갈린다 -> 문장에 대한 확률이 낮다 -> Perplexity가 높다 -> LM 성능이 낮다

<br/>

<br/>



---

다음번에 A Neural Probabilistic Language Model 논문리뷰로 다시 한 번 자세하게 다룰 예정이다.

<br/>

<br/>





