

# [210214 TIL] Language Model ê°œë… ì •ë¦¬

_written by 602_

<br/>



---



## ğŸ“—Language Model ê°œë… ì •ë¦¬

[Reference Link](https://www.youtube.com/watch?v=ycxZORWPPP4&list=PLoYt3dmk_LdhWlr9vPwfZCgivawGtcTjk)



### Language Model



**Statistical Language Model**

- Sparsity problem: ìƒˆë¡œìš´ ë‹¨ì–´ ì¡°í•©ì— ëŒ€í•´ì„œëŠ” í™•ë¥ ê°’ì´ 0ì´ ë˜ëŠ” ë¬¸ì œ



**N-Gram Language Model**

- Uni-gram

  - í•˜ë‚˜ì˜ ë‹¨ì–´ì— ëŒ€í•œ í™•ë¥ ë§Œ ê³„ì‚° -> ì›¬ë§Œí•˜ë©´ í™•ë¥ ê°’ì´ 0ì´ ë˜ì§€ëŠ” ì•ŠìŒ
  - ë‹¨ì–´ì˜ ìˆœì„œ ê³ ë ¤ ëª»í•¨

- Bi-gram

  - ë‘ ë‹¨ì–´ì”© í™•ë¥ ê°’ ê³„ì‚° ($P(ë‚´ì¼|ë­ë¨¹ì§€)$)
  - ì—¬ì „í•œ Sparsity Problem ($P(ë°°ê³ íŒŒ|ì‹œí—˜)$)

  

**Sparsity Problemì˜ ëŒ€ì•ˆ**

- Additive(Laplace) Smoothing: ë‹¨ì–´ë“¤ì˜ countì— 1ì„ ì¶”ê°€í•˜ì—¬ í™•ë¥ ì´ 0ì´ ë˜ëŠ” ê²ƒì„ ë°©ì§€
- Backoff Model:  (Ex) Tri-gramì—ì„œ í™•ë¥ ê°’ì´ 0ì´ ë‚˜ì™”ì„ ê²½ìš° Bi-gramìœ¼ë¡œ ë³€ê²½í•˜ì—¬ í™•ë¥  ê³„ì‚°
- Smoothed(Interpolated) Model: (Ex) Uni,Bi,Tri-gramì˜ í™•ë¥ ê°’ì„ ì„ í˜•ê²°í•©í•˜ì—¬ ìµœì¢… í™•ë¥  ê³„ì‚°

<br/>



### Word Representation



**One-hot Encoding**

- ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ í‘œí˜„í•˜ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•
- ë‹¨ì–´ì˜ ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ìœ„ì¹˜ì— 1, ë‚˜ë¨¸ì§€ë¥¼ 0ìœ¼ë¡œ í• ë‹¹
- (-) ë‹¨ì–´ì˜ ìˆ˜ì— ë¹„ë¡€í•˜ì—¬ ì°¨ì› ì¦ê°€ -> ì—°ì‚° ë¶€ë‹´
- (-) ë‹¨ì–´ ê°„ ê´€ê³„ íŒŒì•… ë¶ˆê°€(ë‘ ë‹¨ì–´ ë²¡í„°ì˜ ë‚´ì ê°’, ì¦‰ ìœ ì‚¬ë„ëŠ” 0) -> ì˜ë¯¸ë‚˜ ë¬¸ë§¥ ë°˜ì˜ ë¶ˆê°€



**Distributed Representation**

- ë¶„ì‚° í‘œìƒ(ì‹¤ìˆ˜ë¡œ ì´ë£¨ì–´ì§„ ë²¡í„°ë¡œ í‘œí˜„)
- Embeddingì´ë¼ê³ ë„ ë¶ˆë¦¼
- (+)ì ì€ ì°¨ì›ìœ¼ë¡œ mapping / ë‹¨ì–´ ê°„ ê´€ê³„ íŒŒì•… ê°€ëŠ¥ / ê°’ì˜ ë³€í™”ì— ëœ ë¯¼ê°



**Embedding layer ì†ŒìŠ¤ ì½”ë“œ(Tensorflow)**

```python
class Embedding(Layer):
  def __init__(self,
input_dim,output_dim,embeddings_initializer='uniform',embeddings_regularizer=None,activity_regularizer=None,embeddings_constraint=None,
mask_zero=False,input_length=None,**kwargs):
    self.input_dim = input_dim
    self.output_dim = output_dim
    self.embeddings_initializer = initializers.get(embeddings_initializer)
    self.embeddings_regularizer = regularizers.get(embeddings_regularizer)
    self.activity_regularizer = regularizers.get(activity_regularizer)
    self.embeddings_constraint = constraints.get(embeddings_constraint)
    self.mask_zero = mask_zero
    self.supports_masking = mask_zero
    self.input_length = input_length
    self._supports_ragged_inputs = True

  @tf_utils.shape_type_conversion
  def build(self, input_shape):
        self.embeddings = self.add_weight(
            shape=(self.input_dim, self.output_dim),
            initializer=self.embeddings_initializer,
            name='embeddings',
            regularizer=self.embeddings_regularizer,
            constraint=self.embeddings_constraint)

  def call(self, inputs):
    out = embedding_ops.embedding_lookup(self.embeddings, inputs)
    return out
```



<br/>



### Neural Probabilistic Language Model



**Purpose**

- For Language Model(ì´í•˜ LM): ë‹¨ì–´ ì‹œí€€ìŠ¤ì— ëŒ€í•œ Joint Probability Functionì„ ë‚˜íƒ€ë‚´ëŠ” ê²ƒ
- For Word Representation: ë‹¨ì–´ì§‘í•© ë‚´ ê° ë‹¨ì–´ì˜ Embedded featureë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒ

- ìœ ì‚¬í•œ êµ¬ì¡°ë‚˜ ì˜ë¯¸ë¥¼ ê°€ì§„ ë¬¸ì¥ì— ëŒ€í•œ ì¼ë°˜í™” ê°€ëŠ¥



**Architecture**

- ì›í•«ë²¡í„° -> Embedding (cf. Lookup Table/Shared Parameter) ->  FCL(hidden size) -> Tanh -> FCL(Vocab size)
- Lookup Table: `Vocab size` x `m(Embedding Size)`
- ëª©í‘œ: Log Likelihoodë¥¼ ìµœëŒ€í™”í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ëŠ” ê²ƒ



**Evaluation Metric: Perplexity**

- ë‹¨ì–´ë“¤ì˜ Joint probabilityì— ì—­ìˆ˜,nì œê³±ê·¼ì„ ì·¨í•œ ê°’

  $\sqrt[n]{\frac{1}{P(W_1,W_2, ..., W_n)}}$

- LMì´ ì£¼ì–´ì§„ ë¬¸ì¥ì„ í—·ê°ˆë ¤í•˜ëŠ” ì •ë„

  - ë¬¸ì¥ì´ í—·ê°ˆë¦°ë‹¤ -> ë¬¸ì¥ì— ëŒ€í•œ í™•ë¥ ì´ ë‚®ë‹¤ -> Perplexityê°€ ë†’ë‹¤ -> LM ì„±ëŠ¥ì´ ë‚®ë‹¤

<br/>

<br/>



---

ë‹¤ìŒë²ˆì— A Neural Probabilistic Language Model ë…¼ë¬¸ë¦¬ë·°ë¡œ ë‹¤ì‹œ í•œ ë²ˆ ìì„¸í•˜ê²Œ ë‹¤ë£° ì˜ˆì •ì´ë‹¤.

<br/>

<br/>





