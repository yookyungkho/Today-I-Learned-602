# [210201 TIL] CS231n Lecture 7 - Training Neural Networks 2

Today I Learned

_written by 602_

<br/>



---

**📕기초과목 제대로 다지기📕**

**: cs231n 복습**



지난 학기, 투빅스 이미지 세미나를 진행하면서 cs231n을 한번 쭉 수강하며 학회원들과 [gitbook](https://tobigs-staff.gitbook.io/-1/)에 자료정리를 했기 때문에, 새로 합류한 딥러닝 스터디에서는 기억해야할 내용 위주로 간략하게 정리하였다.

이번 7강은 내가 리뷰했던 강의이기도 해서 다른 사람의 발표로 다시 접하는 것이 내용을 리마인드하는데 큰 도움이 됐다. 또, 아무래도 석사분이 리뷰해주시다 보니 실제 연구에서 쓰이는 방법론들을 소개받을 수 있어서 좋았다.

 

<br/>



---

# 👀CS231n. Lecture 7) Training Neural Networks 2



## 1. Optimization

- 최적화: 손실 함수(loss funciton)의 최솟값을 찾아나가는 일련의 과정
- 학습률을 적절하게 조정하면서 최솟값을 찾아나가는 것이 중요함



### SGD

- **전체 학습데이터에서 하나씩 랜덤으로 뽑아** 학습을 진행
- Mini-batch SGD: batch_size(k개) 만큼의 데이터를 랜덤으로 뽑아 학습 진행
- (-) local minima, saddle point



### SGD + Momentum

- 자주 쓰이는 방법
- SGD에서 velocity term 추가 > 속도 조절



### NAG

- 이전 스텝의 velocity 방향을 따라 먼저 이동하여 그 자리에서 gradient를 계산하고 다시 본래 자리로 돌아와 actual step을 이동
- 잘 안쓰이는 옵티마이저 중 하나



### AdaGrad

- 기울기 제곱에 반비례하도록 학습률을 조정하여 변동을 줄이는 효과
- 행렬곱 연산으로 가중치마다 다른 학습률 적용



### RMSProp

- 이전 맥락을 고려하여 변동 줄임
- decay rate 추가



### Adam

- 가장 많이 쓰이는 옵티마이저
- momentum과 ada계열의 집합체
- 편향을 잡아주기 위한 장치가 추가되어 보폭, 방향 모두 고려하여 학습률 조정



### learning rate decay

- 요즘은 대부분 cosine decay를 쓰더라

- Adam 대신 SGD+Momentum이 잘 먹히는 경우가 있더라





## 2. Regularization



- 앙상블 관련 자료는 Deep Ensemble, simple weight averaging 논문을 참고하자

- 요즘은 데이터가 워낙 커서  regularization 많이 안쓰는 추세
- Batch norm, data augmentation은 중요
  - 네이버 클로바 cut-mix 논문을 참고하면 augmentation 방법론 나옴
  - 조금 더 창의적인 구글브레인의 auto-augment 논문
  - GAN으로 augmentation하는 논문도 있음





## 3. Transfer Learning

- 너무너무 중요한 Transfer Learning
  - 매번 복잡한 모델을 처음부터 학습시키는건 **비효율**이다
  - 한번 제대로 학습시켜놓은 모델을 가지고 여러 분야에서 수정해서 활용
  - cf) distillation: 거대 모델을 실제로 활용할 수 있게 네트워크를 줄이고 부모 네트워크의 파라미터들을 그대로 받아온 후 fine-tuning
  - Self supervised learning: 네이버가 공을 들이는 영역

---

