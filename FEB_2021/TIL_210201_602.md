# [210201 TIL] CS231n Lecture 7 - Training Neural Networks2

_written by 602_

<br/>



---





- 라이브러리: wandb ray tune



- natural gradient



- 요즘은 대부분 cosine decay를 쓰더라



- Adam 대신 SGD+Momentum이 잘 먹히는 경우가 있더라



- 앙상블 관련 자료는 Deep Ensemble, simple weight averaging 참고
- 요즘은 데이터가 워낙 커서  regularization 많이 안씀. Batch norm, data augmentation은 중요
  - 네이버 클로바 cut-mix 논문을 참고하면 augmentation 방법론 나옴
  - 조금 더 창의적인 구글브레인의 auto-augment 논문
  - GAN으로 augmentation하는 논문도 있음



- 너무너무 중요한 Transfer Learning
  - 매번 그 복잡한 모델을 처음부터 학습시키는건 비효율이다
  - 한번 제대로 학습시켜놓은 모델을 가지고 여러 분야에서 수정해서 활용
  - cf) distillation: 거대 모델을 실제로 활용할 수 있게 네트워크를 줄이고 부모 네트워크의 파라미터들을 그대로 받아온 후 fine-tuning
  - Self supervised learning: 네이버가 공을 들이는 영역

---

